# -*- coding: utf-8 -*-
"""textclassy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sylJ6c__ebCH_SuM_pupFWEAq1aIUNcN

#**1) Mount your google drive in google colab:**
"""

from google.colab import drive
drive.mount('/content/gdrive/')

"""#**2) Append the directory to your python path using sys:**"""

import sys
sys.path.append('/content/gdrive/MyDrive/Flood_Data_Work_2022')

"""#**Import essential Library**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn

"""**Take datasets from event_task_2 read 1 training datasest:kerala_floods_2018**"""

old_data=pd.read_csv('/content/gdrive/MyDrive/Flood_Data_Work_2022/HumAID_data_events_set2_29K-20221011T035940Z-001/HumAID_data_events_set2_29K/events_set2/kerala_floods_2018/kerala_floods_2018_train.tsv',sep='\t')

"""## **Check Last 10 Rows of The Dataset**"""

old_data.head()

"""## **Find Shape of Our Dataset (Number of Rows And Number of Columns)**"""

old_data.shape

print("Number of Rows", old_data.shape[0])
print("Number of Colum", old_data.shape[1])

"""# **Getting Information About Our Dataset Like Total Number Rows, Total Number of Columns, Datatypes of Each Column And Memory Requirement**"""

old_data.info()

"""# **Check Missing Values In The Dataset**"""

print("Any missing value?",old_data.isnull().values.any())

#old_data.isnull()

old_data.isnull().sum()

sns.heatmap(old_data.isnull())

per_missing=old_data.isnull().sum()*100/len(old_data)
print(per_missing)

"""## **Drop All The Missing Values**"""

#old_data.dropna(axis=0)

#old_data.dropna(axis=0,inplace=True)

#old_data.dropna()

"""# **Check For Duplicate Data**"""

dup_data=old_data.duplicated().any()

print("Are there any Duplicate Values?",dup_data)

'''old_data=old_data.drop_duplicates()
old_data'''

"""# **Get Overall Statistics About The DataFrame**"""

old_data.describe()

old_data.describe(include='all')
#for categorical and as well numerical column.

"""# **Find Class in Class_label**"""

old_data.columns

old_data.class_label.unique()

print(len(old_data.class_label.unique()))

old_data['class_label'].value_counts()

'''#old_data['tweet_class_label'] = 'NAN'
old_data.insert(3, "tweet_class_label",'NaN', allow_duplicates=False)
old_data'''

# Shuffling a Pandas dataframe with sklearn
from sklearn.utils import shuffle
shuffled_old_data = shuffle(old_data, random_state=100)
#print(shuffled_old_data.head())
shuffled_old_data.head()

"""#**Text Preprocessing And Data Cleanning**


> *Text Preprocessing*

---
#-------------------------
Expand Contractions
Remove Extra Spaces
Removal of Emojis
Removal of Emoticons
Conversion of Emoticon to Words
Conversion of Emoji to Words
Removing Accented Characters
Removal of URLs
Removal of HTML Tags
Chat Words Conversion
Spelling Correction
Rephrase text #-------------------
Removing Special Characters
Remove Non-English Words
#------------------------------

# **Text Preprocessing and corresponding library**
"""

import re # relugar expression
#import nltk # natural language toolkit
import string

"""#**1-Expand Contractions**"""

'''contraction_dict = {"ain't": "are not","'s":" is","aren't": "are not"}
# Regular expression for finding contractions
contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text,contractions_dict=contractions_dict):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)
# Expanding Contractions in the reviews
data['reviews.text']=data['reviews.text'].apply(lambda x:expand_contractions(x))'''
#---------------------------------------------------------
# imports
from contractions import CONTRACTION_MAP # from contractions.py
import re 
# function to expand contractions
def expand_contractions(text, map=CONTRACTION_MAP):
    pattern = re.compile('({})'.format('|'.join(map.keys())), flags=re.IGNORECASE|re.DOTALL)
    def get_match(contraction):
        match = contraction.group(0)
        first_char = match[0]
        expanded = map.get(match) if map.get(match) else map.get(match.lower())
        expanded = first_char+expanded[1:]
        return expanded 
    new_text = pattern.sub(get_match, text)
    new_text = re.sub("'", "", new_text)
    return new_text
# call function 
#expand_contractions("Y'all i'd contractions you're expanded don't think.")
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["tweet_text"].apply(lambda x: expand_contractions(x))
shuffled_old_data["new_tweet_text"]

shuffled_old_data.head()

"""#**2-Remove Extra Spaces**"""

#---------
# imports
import re
# function to remove special characters
def remove_extra_whitespace_tabs(text):
    #pattern = r'^\s+$|\s+$'
    pattern = r'^\s*|\s\s*'
    return re.sub(pattern, ' ', text).strip()
# call function
#text = '  This web line  has \t some extra  \t   tabs and whitespaces  '
#remove_extra_whitespace_tabs(text)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_extra_whitespace_tabs(x))
shuffled_old_data["new_tweet_text"]
#-----------------------------------

'''
#shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: re.sub(' +', ' ', x))
shuffled_old_data["new_tweet_text"] = shuffled_old_data["tweet_text"].apply(lambda x: re.sub(r'\s+', ' ', x))
shuffled_old_data["new_tweet_text"]
#shuffled_old_data["new_tweet_text"] = re.sub(r'\s+', ' ', shuffled_old_data["new_tweet_text"])
'''

"""# **3-Removal of Emojis**"""

def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_emoji(x))
shuffled_old_data["new_tweet_text"]

"""# **4-Removal of Emoticons**"""

# imports
from emoticons import EMOTICONS # from emoticons.py
import re 
# function to Removal of Emoticons

def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text)

#remove_emoticons("Hello :-)")
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_emoticons(x))
shuffled_old_data["new_tweet_text"]

"""#**5-Conversion of Emoticon to Words**"""

def convert_emoticons(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
    return text

#text = "Hello :-) :-)"
#convert_emoticons(text)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: convert_emoticons(x))
shuffled_old_data["new_tweet_text"]

"""#**6-Conversion of Emoji to Words**"""

'''from emojitowords import UNICODE_EMOJI
print(UNICODE_EMOJI)'''

#---------------------------------------------------------
# imports
#from emot.emo_unicode import UNICODE_EMOJI # For emojis
#from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS
#------------------------------
from emojitowords import UNICODE_EMOJI # from emojitowords.py
import re 
# function to Conversion of Emoji to Words

def convert_emojis(text):
  for emot in UNICODE_EMOJI:
    text = re.sub(r'('+emot+')', "_".join(UNICODE_EMOJI[emot].replace(",","").replace(":","").split()), text)
    return text

#text = "game is on üî•"
#convert_emojis(text)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: convert_emojis(x))
shuffled_old_data["new_tweet_text"]

"""#**7-Removing Accented Characters**"""

# imports
import unicodedata
# function to remove accented characters
def remove_accented_chars(text):
    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return new_text
# call function
#remove_accented_chars('S√≥mƒõ √Åccƒõntƒõd tƒõxt. Some words such as r√©sum√©, caf√©, pr√≥test, divorc√©, co√∂rdinate, expos√©, latt√©.')
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_accented_chars(x))
shuffled_old_data["new_tweet_text"]

"""#**8-Removal of URLs**"""

def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)
#text = "Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/"
#remove_urls(text)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_urls(x))
shuffled_old_data["new_tweet_text"]

"""#**9-Removal of HTML Tags**"""

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)

'''text = """<div>
<h1> H2O</h1>
<p> AutoML</p>
<a href="https://www.h2o.ai/products/h2o-driverless-ai/"> Driverless AI</a>
</div>"""
print(remove_html(text))
'''
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_html(x))
shuffled_old_data["new_tweet_text"]

"""#**10-Chat Words Conversion**"""

'''from cwc import chat_words_str
print(chat_words_str)'''

#------------------------------
from cwc import chat_words_str # from cwc.py
import re 
# function to Chat Words Conversion
#-------------------------------------

chat_words_map_dict = {}
chat_words_list = []
for line in chat_words_str.split("\n"):
    if line != "":
        cw = line.split("=")[0]
        cw_expanded = line.split("=")[1]
        chat_words_list.append(cw)
        chat_words_map_dict[cw] = cw_expanded
chat_words_list = set(chat_words_list)

def chat_words_conversion(text):
    new_text = []
    for w in text.split():
        if w.upper() in chat_words_list:
            new_text.append(chat_words_map_dict[w.upper()])
        else:
            new_text.append(w)
    return " ".join(new_text)

#chat_words_conversion("one minute BRB")
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: chat_words_conversion(x))
shuffled_old_data["new_tweet_text"]

"""#**11-Spelling Correction**"""

!pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()
def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)
        
#text = "speling correctin"
#correct_spellings(text)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["tweet_text"] = shuffled_old_data["tweet_text"].apply(lambda x: chat_words_conversion(x))
shuffled_old_data["tweet_text"]

"""#**12-Rephrase text**"""

'''#email-id
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"]  = shuffled_old_data["new_tweet_text"] .apply(lambda x: re.sub('b[w-.]+?@w+?.w{2,4}b', 'emailadd',x))
#url
shuffled_old_data["new_tweet_text"]  = shuffled_old_data["new_tweet_text"] .apply(lambda x:re.sub('(http[s]?S+)|(w+.[A-Za-z]{2,4}S*)', 'urladd', x))
'''

"""#**13-Removing Special Characters**"""

# imports
import re
# function to remove special characters
def remove_special_characters(text):
    # define the pattern to keep
    pat = r'[^a-zA-z0-9.,!?/:;\"\'\s]' 
    return re.sub(pat, '', text)
 
# call function
#remove_special_characters("007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!")
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_special_characters(x))
shuffled_old_data["new_tweet_text"]

"""#**14-Remove Non-English Words**"""

import nltk # natural language toolkit
nltk.download('words')

def removeNonEnglishWordsFunct(x):
    words = set(nltk.corpus.words.words())
    filteredSentence = " ".join(w for w in nltk.wordpunct_tokenize(x) \
                                if w.lower() in words or not w.isalpha())
    return filteredSentence


string = "NLTK testing man Apple Confiz Burj Al Arab Copacabana Palace Ôº∑„ÅØÊØîËºÉÁöÑÊñ∞„Åó„Åè„Å¶„Åç„Çå„ÅÑ„Å™„ÅÆ„Åß„Åô„Åå Ôº≥ÔΩàÔΩÖÔΩíÔΩÅÔΩîÔΩèÔΩé hotel„ÅØÊôÇ„ÄÖ ÔºÆÔºπ„Çâ„Åó„ÅÑÂ∞è„Åï„Åè„Å¶Ê∏ÖÊΩîÊÑü„ÅÆ„Å™„ÅÑÈÉ®Â±ã"

#res = removeNonEnglishWordsFunct(string)
#print(res)
#-------
#---------------- From file data
#-----------------
shuffled_old_data["new_tweet_text"] = shuffled_old_data["new_tweet_text"].apply(lambda x: remove_special_characters(x))
shuffled_old_data["new_tweet_text"]

#---------------------
'''
import nltk
words = set(nltk.corpus.words.words())

sent = "Io andiamo to the beach with my amico."
" ".join(w for w in nltk.wordpunct_tokenize(sent) \
         if w.lower() in words or not w.isalpha())
# 'Io to the beach with my'
'''

shuffled_old_data.head()

"""#**Binary Classification**

> *Add First New Column on basis of Data label classification*

> *Add Second New Column on basis of Data label classification*

#**Add First New Column on basis of Data label classification**
"""

def categorise_1(row):  
    if row['class_label'] =='rescue_volunteering_or_donation_effort':
        return 'situational_non_informative'
    elif row['class_label'] =='other_relevant_information':
        return 'non_situational_informative'
    elif row['class_label'] =='sympathy_and_support':
        return 'non_situational_non_informative'
    elif row['class_label'] =='requests_or_urgent_needs':
        return 'situational_informative'
    elif row['class_label'] =='not_humanitarian':
        return 'non_situational_non_informative'
    elif row['class_label'] =='injured_or_dead_people':
        return 'situational_informative'
    elif row['class_label'] =='infrastructure_and_utility_damage':
        return 'situational_informative'
    elif row['class_label'] =='caution_and_advice':
        return 'situational_informative'
    elif row['class_label'] =='displaced_people_and_evacuations':
        return 'situational_informative'
    return 'NAN'

shuffled_old_data['tweet_class_label'] = shuffled_old_data.apply(lambda row: categorise_1(row), axis=1)
shuffled_old_data.head()

"""**Find NaN Values**"""

nan_in_df = shuffled_old_data.isnull().values.any()
print(nan_in_df)

shuffled_old_data.isna().any()[lambda x: x]

shuffled_old_data.loc[:, shuffled_old_data.isna().any()]

"""#**Add Second New Column on basis of Data label classification**"""

shuffled_old_data.columns

shuffled_old_data.tweet_class_label.unique()

print(len(shuffled_old_data.tweet_class_label.unique()))

shuffled_old_data['tweet_class_label'].value_counts()

def categorise_2(row):  
    if row['tweet_class_label'] =='situational_non_informative':
        return 'non_informative'
    elif row['tweet_class_label'] =='non_situational_informative':
        return 'non_informative'
    elif row['tweet_class_label'] =='non_situational_non_informative':
        return 'non_informative'
    elif row['tweet_class_label'] =='situational_informative':
        return 'informative'
    return 'NAN'

shuffled_old_data['category_class_label'] = shuffled_old_data.apply(lambda row: categorise_2(row), axis=1)
shuffled_old_data.head()

"""**Find NaN Values**"""

nan_in_df = shuffled_old_data.isnull().values.any()
print(nan_in_df)

shuffled_old_data.isna().any()[lambda x: x]

shuffled_old_data.loc[:, shuffled_old_data.isna().any()]

"""#**Information**"""

shuffled_old_data.columns

shuffled_old_data.category_class_label.unique()

print(len(shuffled_old_data.category_class_label.unique()))

shuffled_old_data['category_class_label'].value_counts()

shuffled_old_data.head(10)

